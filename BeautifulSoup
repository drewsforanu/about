import asyncio
import aiohttp
from bs4 import BeautifulSoup

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def scrape(url):
    async with aiohttp.ClientSession() as session:
        html = await fetch(session, url)
        soup = BeautifulSoup(html, 'html.parser')
        print(f"Title of {url}: {soup.title.string}")

urls = ['https://example.com', 'https://python.org', 'https://github.com']

async def main():
    tasks = [scrape(url) for url in urls]
    await asyncio.gather(*tasks)

asyncio.run(main())
